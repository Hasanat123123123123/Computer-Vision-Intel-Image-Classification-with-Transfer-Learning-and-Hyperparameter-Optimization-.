# -*- coding: utf-8 -*-
"""Home_task_07.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CZUjLWI1wAJ5bu31sL1aVe7azJawGK5G
"""

import torch
import torchvision.transforms as transforms
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader, random_split
from torchvision.models import resnet18, efficientnet_b0
import pytorch_lightning as pl
from pytorch_lightning import Trainer
from pytorch_lightning.loggers import WandbLogger
import optuna
from optuna.integration.pytorch_lightning import PyTorchLightningPruningCallback
import os
import wandb

!pip install optuna-integration[pytorch_lightning]

!pip install optuna-integration[pytorch_lightning]

from google.colab import files

# Upload the dataset file
uploaded = files.upload()

!unzip "archive (1).zip" -d /content/dataset/

wandb.login()

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Data loading and transformations
import os
from torchvision import transforms
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader, random_split
import pytorch_lightning as pl
import torch
from torchvision.models import resnet18, efficientnet_b0
from pytorch_lightning.loggers import WandbLogger
from pytorch_lightning import Trainer
import optuna
from optuna.integration.pytorch_lightning import PyTorchLightningPruningCallback

data_dir = "/content/dataset"  # Adjust the path if necessary

# Update the dataset paths based on your folder names
train_dir = os.path.join(data_dir, "seg_train")
test_dir = os.path.join(data_dir, "seg_test")
prediction_dir = os.path.join(data_dir, "seg_pred")

# Define transformations
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# Load dataset
train_dataset = ImageFolder(train_dir, transform=transform)
test_dataset = ImageFolder(test_dir, transform=transform)
prediction_dataset = ImageFolder(prediction_dir, transform=transform)

# Split the train dataset into training and validation sets
train_size = int(0.8 * len(train_dataset))
val_size = len(train_dataset) - train_size
train_data, val_data = random_split(train_dataset, [train_size, val_size])

# Data loaders
train_loader = DataLoader(train_data, batch_size=32, shuffle=True)
val_loader = DataLoader(val_data, batch_size=32)
test_loader = DataLoader(test_dataset, batch_size=32)
prediction_loader = DataLoader(prediction_dataset, batch_size=32)

# Define the PyTorch Lightning Module
class ImageClassifier(pl.LightningModule):
    def __init__(self, backbone='resnet18', learning_rate=1e-3, num_classes=6):
        super().__init__()
        self.learning_rate = learning_rate
        if backbone == 'resnet18':
            self.model = resnet18(pretrained=True)
            self.model.fc = torch.nn.Linear(self.model.fc.in_features, num_classes)
        elif backbone == 'efficientnet_b0':
            self.model = efficientnet_b0(pretrained=True)
            self.model.classifier[1] = torch.nn.Linear(self.model.classifier[1].in_features, num_classes)
        else:
            raise ValueError("Unsupported backbone")

        self.criterion = torch.nn.CrossEntropyLoss()

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.criterion(y_hat, y)
        self.log('train_loss', loss)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.criterion(y_hat, y)
        acc = (y_hat.argmax(dim=1) == y).float().mean()
        self.log('val_loss', loss, prog_bar=True)
        self.log('val_acc', acc, prog_bar=True)
        return loss

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)

# Hyperparameter Optimization using Optuna
def objective(trial):
    backbone = trial.suggest_categorical("backbone", ["resnet18", "efficientnet_b0"])
    learning_rate = trial.suggest_float("learning_rate", 1e-5, 1e-2, log=True)

    model = ImageClassifier(backbone=backbone, learning_rate=learning_rate)

    wandb_logger = WandbLogger(project="Intel_Image_Classification")

    trainer = Trainer(
        max_epochs=5,
        logger=wandb_logger,
        devices=1,  # Use 1 device (CPU or GPU)
        accelerator='cpu'  # Use CPU
    )

    pruning_callback = PyTorchLightningPruningCallback(trial, monitor="val_loss")
    trainer.callbacks.append(pruning_callback)

    trainer.fit(model, train_loader, val_loader)

    return trainer.callback_metrics["val_loss"].item()

# Create the study and optimize the objective
study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=10)

# Train with the best hyperparameters
best_params = study.best_params
model = ImageClassifier(backbone=best_params["backbone"], learning_rate=best_params["learning_rate"])

wandb_logger = WandbLogger(project="Intel_Image_Classification")
trainer = Trainer(max_epochs=20, logger=wandb_logger, gpus=1 if torch.cuda.is_available() else 0)
trainer.fit(model, train_loader, val_loader)

# Evaluate on Test and Prediction Data
test_results = trainer.test(model, test_loader)
prediction_results = trainer.predict(model, prediction_loader)

print("Test Results:", test_results)
print("Prediction Results:", prediction_results)